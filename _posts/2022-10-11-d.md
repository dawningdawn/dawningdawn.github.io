---
layout: post
title:  "Model Review:Transformer"
date:   2022-10-11
categories: writing
published: true
---

* Sequence transduction models are the models that transduce sequences.
* What is transduction? Transduction is “deriving the values of the unknown function for points of interest from the given data” whcih could be understood as “moving from the particular to the particular.” [1]
* Hence, sequence transduction models are essentially trying to predict sequential data points from sequential data points, which presumably belong to the same unknown function.
* The authors say that “dominant sequence transduction models are based on complex RNNs or CNNS that include an encoder and a decoder” and “the best performing models also connect the encoder and decoder through an attention mechanism.”
* By this, we can assume that a successful attention model requires an encoder and decoder connected through an attention mechanism.
* Possilby inspired by the trend, the authors propose a new **network architecture** “Transformer” which is “based soley on attention mechanisms” without recurrence and convolutions.
* Below are the advertised advantages evidenced by the results of experimets on two machine learning translation.

| Advantages      |
|-----------|
| Superior in quality   |
| More parallelizable   |
| Requiring significantly less time to train   |

| Experiments on machine learning translation | Results | Training Conditions | Note |
|-------------------|-----------------|------|------|
| WMT 2014 English-to-German translation task | 28.4 BLEU | ? | Improves over the existing best results, including ensembles, by over 2 BLEU |
| WMT 2014 English-to-French translation task | 41.0 BLEU | 3.5 days on eight GPUs | Small fraction of the training costs of the best models from the literature |

Let's have a look at the model architecture.
![](http://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png)

* Encoder
    * Takes as input a sequence of symbol representations (x_1, ..., x_n) and map it onto a seqeunce of continuous representations z=(z_1,...,z_n).
    * A stack of N=6 identical layers, each layer having 2 sub-layers.
        * 1st sub-layer: A multi-head self-attention mechanism
        * 2nd sub-layer: A simple, position-wise fully connected feed-forward network*.
        <small>*A type of feedforward layer consisting of two dense layers that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise.</small>
        * Each of the 2 sub-layers has a residual connection** around them, followed by layer nomalization***.
        <small>**A type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions.</small>
        <small>***Unlike batch normalization, directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases.

* Decoder
    * Takes as input $z$ and generates an output sequence of symbols (y_1,...,y_m), one element at a time. 
    * A stack of N=6 identical layers, each layer having 3 sub-layers.
        * 2 Sub-layers: Same as above. Has a residual connection around them, followed by layer nomalization.
        * 3rd sub-layer: Performing multi-head attention over the output of the encoder stack.
            * Modified to prevent postions from attending to subsequent positions.
            * “This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.”
* “At each step the model is auto-regressive*, consuming the previously generated symbols as additional input when generating the next.” (p.2)
    * autorgressive model*: A representation of a type of random process, used to describe certain time-varying processes in nature, economics, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term). Thus the model is in the form of a stochastic difference equation (or recurrence relation which should not be confused with differential equation). 
    * Do we think and speak in auto-regressive manner?
* “Stacked, self-attention and point-wise*, fully connected layers for both the encoder and decoder”
    * point-wise* : Used to indicate that a certain property is defined by considering each value f(x) of some function f. (It means that when doing operations between functions like (f+g(x), the operation should be applied to data points in each of the functions. Hence, (f+g)(x)=f(x)+g(x). cf. convolution operations on functions is NOT pointwise.
* Attention 
    * function: Maps a query and a set of key-value pairs to an output.
        * Query, keys, values, outputs: all vectors. 
        * Output: computed weigted sum of the values, where the weight assigned to each value is computed by a compatibility function* of the query with the corresponding key.
            * compatibility function: Typically, a neural network parameterized by weights 𝐖 is a function from an input 𝑥 to an output 𝑦.
            * The network has an associated compatibility function Ψ(𝑦;𝑥,𝐖) → ℝ^+ that measures how likely an output y is given an input x under weights 𝑊.
    
# References
[Attention is all you need](https://arxiv.org/abs/1706.03762)
<br>[1](https://link.springer.com/book/10.1007/978-1-4757-3264-1) The Nature of Statistical Learning Theory, 1995, p.169 

<!-- %enddocs -->

## License

[MIT](./LICENSE)
