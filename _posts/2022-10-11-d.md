---
layout: post
title:  "Model Review:Transformer"
date:   2022-10-11
categories: writing
published: true
---

* In this post, I will review the deep learning model **Transformer** which was introduced by Google Brain in the 2017 paper "Attention Is All You Need."
* This paper introduces a sequence transduction model, which is a model that tranduces sequences.
* What is transduction? Transduction is â€œ**deriving the values of the unknown function for points of interest from the given data**â€ which is simply put, â€œ**moving from the particular to the particular.**â€ [1]
* Hence, sequence transduction models are essentially trying to predict sequential data points from sequential data points, which presumably belong to the same unknown function.
* The authors say that â€œdominant sequence transduction models are based on complex RNNs or CNNS that include an encoder and a decoderâ€ and â€œthe best performing models also connect the encoder and decoder through an attention mechanism.â€
* By this, we can see that in 2017, successful sequence transduction models have an encoder and decoder.
* Possilby inspired by the trend, the authors propose a new network architecture â€œTransformerâ€ which is â€œbased soley on attention mechanismsâ€ without recurrence and convolutions.
* Below are the advertised advantages evidenced by the results of experimets on two machine learning translation.

  | Advantages      |
  |-----------|
  | Superior in quality   |
  | More parallelizable   |
  | Requiring significantly less time to train   |

  | Experiments on machine learning translation | Results | Training Conditions | Note |
  |-------------------|-----------------|------|------|
  | WMT 2014 English-to-German translation task | 28.4 BLEU | ? | Improves over the existing best results, including ensembles, by over 2 BLEU |
  | WMT 2014 English-to-French translation task | 41.0 BLEU | 3.5 days on eight GPUs | Small fraction of the training costs of the best models from the literature |

* Let's have a look at the model architecture.
  ![](http://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png)
* The model is largely comprised of three parts: encoder, decoder, and attention. Let's look at them one by one.

# Encoder
* The encoder takes as input **a sequence of symbol representations** (x<sub>1</sub>, ..., x<sub>n</sub>) and map it onto **a seqeunce of continuous representations** z = (z<sub>1</sub>, ..., z<sub>n</sub>).
* It is made of a stack of N = 6 identical layers, each layer having 2 sub-layers.
    * 1st sub-layer: A multi-head self-attention mechanism
    * 2nd sub-layer: A simple, **position-wise fully connected feed-forward network**.
        
      <small>**Position-wise Feed-forward Layer**: "A type ofÂ feedforward layerÂ consisting of twoÂ **dense layers**Â that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise." [2]</small>
      
      <small>**Dense Connections** (Fully Connected Connections): "A type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are n<sub>inputs</sub> <span>&#215;</span> n<sub>outputs</sub> parameters, which can lead to a lot of parameters for a sizeable network." [3]</small> 
      
      <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_6.31.32_PM_xBfVMWZ.png" 
           width="315" 
           height="154.5" />   
      <small>Image Source: Deep Learning by Goodfellow, Bengio and Courville</small>
        
* Each of the 2 sub-layers has a **residual connection** around them, followed by **layer nomalization**.
        
  <small>**Residual Connection**: "A type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions.
  <br>Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers fit another mapping of F(x) := H(x) - x. The original mapping is recast into F(x) + x.
  <br>The intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers." [4]</small>  
  
  <img src="https://production-media.paperswithcode.com/methods/resnet-e1548261477164.png" 
       width="282.5" 
       height="158.75" />           
  <small>Image Source: Deep Residual Learning for Image Recognition</small>
        
  <small>**Layer normalization**: "Unlike batch normalization, directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for RNNs and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with Transformer models." [5]</small>
  <br><img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png" 
       width="226" 
       height="173" />     
  <small>Image Source: Paperswithcode -- Layer Normalization</small>

# Decoder
* Takes as input z and generates an output sequence of symbols (y<sub>1</sub>, ... , y<sub>m</sub>), one element at a time. 
* A stack of N = 6 identical layers, each layer having 3 sub-layers.
    * 2 sub-layers: Same as above. Has a residual connection around them, followed by layer nomalization.
    * 3rd sub-layer: Performing multi-head attention over the output of the encoder stack.
        * Modified to prevent postions from attending to subsequent positions.
        * â€œThis masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.â€
* â€œAt each step the model is **auto-regressive**, consuming the previously generated symbols as additional input when generating the next.â€ (p.2)
   
  <small>**Auto-regressive model**: A representation of a type of random process, used to describe certain time-varying processes in nature, economics, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term). Thus the model is in the form of a stochastic difference equation (or recurrence relation which should not be confused with differential equation). [6]</small>  
   
  <small>--But do we think and speak in auto-regressive manner?</small>

* â€œStacked, self-attention and **point-wise**, fully connected layers for both the encoder and decoderâ€

  <small>**Point-wise**: "Used to indicate that a certain property is defined by considering each value f(x) of some function f. It means that when doing operations between functions like (f+g)(x), the operation should be applied to data points in each of the functions. Hence, (f+g)(x) = f(x) + g(x). cf. convolution operations on functions is NOT pointwise." [7]</small>

# Attention 
* Function: Maps a query and a set of key-value pairs to an output.
    * Query, keys, values, outputs: all vectors. 
    * Output: computed weigted sum of the values, where the weight assigned to each value is computed by a **compatibility function** of the query with the corresponding key.
   
      <small>**Compatibility Function**: "Typically, a neural network parameterized by weightsÂ ğ–Â is a function from an inputÂ ğ‘¥Â to an outputÂ ğ‘¦. The network has an associated compatibility functionÂ Î¨(ğ‘¦; ğ‘¥, ğ–) â†’ â„<sup>+</sup>Â that measures how likely an output ğ‘¦ is given an input ğ‘¥ under weightsÂ ğ‘Š." [8] </small>
    
# References
[Attention is all you need](https://arxiv.org/abs/1706.03762)
<br>[1] [The Nature of Statistical Learning Theory, 1995, p.169](https://link.springer.com/book/10.1007/978-1-4757-3264-1)
<br>[2] [Paperswithcode -- Position-Wise Feed-Forward Layer](https://paperswithcode.com/method/position-wise-feed-forward-layer)
<br>[3] [Paperswithcode -- Dense Connections](https://paperswithcode.com/method/dense-connections)
<br>[4] [Paperswithcode -- Residual Connection](https://paperswithcode.com/method/residual-connection)
<br>[5] [Paperswithcode -- Layer Normalization](https://paperswithcode.com/method/layer-normalization)
<br>[6] [Wikipedia -- Autoregressive Model](https://en.wikipedia.org/wiki/Autoregressive_model)
<br>[7] [Wikipedaia -- Pointwise](https://en.wikipedia.org/wiki/Pointwise)
<br>[8] [Gradient-based Inference for Networks with Output Constraints, AAAI 2019](https://arxiv.org/abs/1707.08608)

<!-- %enddocs -->

## License

[MIT](./LICENSE)
